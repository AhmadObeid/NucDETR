{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2917e9e2-3f06-416a-b30e-2948cdb22bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../FromPublic/detr',\n",
       " '/home/nofelvsap/PhD-Ahmad Obeid/ColorectalCancer/NucDETR/PythonScripts',\n",
       " '/home/nofelvsap/anaconda3/envs/nucdetr/lib/python310.zip',\n",
       " '/home/nofelvsap/anaconda3/envs/nucdetr/lib/python3.10',\n",
       " '/home/nofelvsap/anaconda3/envs/nucdetr/lib/python3.10/lib-dynload',\n",
       " '',\n",
       " '/home/nofelvsap/anaconda3/envs/nucdetr/lib/python3.10/site-packages',\n",
       " '/home/nofelvsap/anaconda3/envs/nucdetr/lib/python3.10/site-packages/PyQt5_sip-12.11.0-py3.10-linux-x86_64.egg',\n",
       " '/home/nofelvsap/anaconda3/envs/nucdetr/lib/python3.10/site-packages/PyYAML-6.0-py3.10-linux-x86_64.egg',\n",
       " '/home/nofelvsap/anaconda3/envs/nucdetr/lib/python3.10/site-packages/IPython/extensions',\n",
       " '/home/nofelvsap/.ipython']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'../../FromPublic/detr')\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f0085ab-ad5a-4ab8-b56e-b2c8f9fb0bea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from models import backbone\n",
    "from models.backbone import Backbone\n",
    "from util.misc import NestedTensor, interpolate, nested_tensor_from_tensor_list\n",
    "from models import build_model\n",
    "from datasets import build_dataset\n",
    "import util.misc as utils\n",
    "from ipywidgets import FloatProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "479c0014-339e-46c4-a590-e7e35f3b5b24",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pdb\n",
    "from PIL import Image, ImageDraw\n",
    "import requests\n",
    "import io\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "from torchvision.models import resnet50, resnet101, resnet18, resnet34\n",
    "import torchvision.transforms as T\n",
    "from torchvision import datasets\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import numpy\n",
    "# torch.set_grad_enabled(False);\n",
    "\n",
    "from torchvision import models\n",
    "\n",
    "import scipy.io as sio\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "import argparse\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2684d5ad-3bbb-48b5-9179-871786c44625",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cell ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c4dd7bb-3f48-4672-a427-6105818f141f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/nofelvsap/.cache/torch/hub/facebookresearch_detr_main\n",
      "/home/nofelvsap/anaconda3/envs/nucdetr/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/nofelvsap/anaconda3/envs/nucdetr/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Using cache found in /home/nofelvsap/.cache/torch/hub/facebookresearch_detr_main\n"
     ]
    }
   ],
   "source": [
    "def get_pretrained_detr():\n",
    "  model_original = torch.hub.load('facebookresearch/detr', 'detr_resnet101', pretrained=True)\n",
    "  model_original.eval();\n",
    "  model = torch.hub.load('facebookresearch/detr', 'detr_resnet101', pretrained=False, num_classes=2)\n",
    "  model.eval();\n",
    "\n",
    "  # change the name of class_embedd to be able to import the weights with different size of last layer\n",
    "  dic = model_original.state_dict()\n",
    "  dic['class_embed.new_weight'] = dic['class_embed.weight']\n",
    "  dic['class_embed.new_bias'] = dic['class_embed.bias']\n",
    "  del dic['class_embed.weight'], dic['class_embed.bias']\n",
    "  model.load_state_dict(dic, strict=False)\n",
    "  return model\n",
    "model = get_pretrained_detr()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51f533d-2313-430b-9703-8ef81d20aaf5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Backbone: Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7912562-b7ad-482e-aa63-c52b8f5eeb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Negative', 'Positive']\n"
     ]
    }
   ],
   "source": [
    "# data_dir_tr = '/content/drive/My Drive/Colab/CRC/datasets/consep/tr'\n",
    "# data_dir_ts = '/content/drive/My Drive/Colab/CRC/datasets/consep/ts'\n",
    "data_dir_tr = '../../Datasets/AllTogether/train'\n",
    "data_dir_ts = '../../Datasets/AllTogether/val'\n",
    "def load_split_train_test(datadir_tr,datadir_ts,batch_size):\n",
    "    train_transforms = T.Compose([T.ToTensor(),\n",
    "                                  T.Resize(300)])\n",
    "    test_transforms = T.Compose([T.ToTensor(),\n",
    "                                 T.Resize(300)])\n",
    "    train_data = datasets.ImageFolder(datadir_tr,       \n",
    "                    transform=train_transforms)\n",
    "    test_data = datasets.ImageFolder(datadir_ts,\n",
    "                    transform=test_transforms)\n",
    "    # num_train = len(train_data)\n",
    "    # indices = list(range(num_train))\n",
    "    # split = int(numpy.floor(valid_size * num_train))\n",
    "    # numpy.random.shuffle(indices)\n",
    "    # from torch.utils.data.sampler import SubsetRandomSampler\n",
    "    # train_idx, test_idx = indices[split:], indices[:split]\n",
    "    # train_sampler = SubsetRandomSampler(train_idx)\n",
    "    # test_sampler = SubsetRandomSampler(test_idx)\n",
    "    trainloader = torch.utils.data.DataLoader(train_data,batch_size=batch_size,shuffle=True)\n",
    "    testloader = torch.utils.data.DataLoader(test_data,batch_size=batch_size)\n",
    "    return trainloader, testloader\n",
    "batch_size = 32\n",
    "trainloader, testloader = load_split_train_test(data_dir_tr,data_dir_ts,batch_size)\n",
    "print(trainloader.dataset.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88545992-5ca5-489a-a56c-56617ebac09e",
   "metadata": {},
   "source": [
    "## Backbone: making model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cad9559-3db5-43a4-8241-84a6a5c91580",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() \n",
    "                                  else \"cpu\")\n",
    "torchMdl = resnet101(pretrained=True,progress=True)\n",
    "torchMdl.fc = nn.Sequential(nn.Linear(2048, 512),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(0.2),\n",
    "                                 nn.Linear(512, 2))\n",
    "                                  #nn.Sigmoid() , nn.LogSoftmax(dim=1) nn.LogSoftmax(dim=1)\n",
    "                                 \n",
    "# for freezing if needed\n",
    "# for n,p in torchMdl.named_parameters():\n",
    "#   if \"fc\" in n or \"layer4\" in n:\n",
    "#     p.requires_grad = True\n",
    "#   else:\n",
    "#     p.requires_grad = False\n",
    "# end of freezing \n",
    "\n",
    "criterion = nn.CrossEntropyLoss() #\n",
    "optimizer = optim.SGD(torchMdl.parameters(), lr=0.01, momentum=0.9)\n",
    "torchMdl.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825f8a2f-1848-4261-8aa7-a51e0254ea60",
   "metadata": {},
   "outputs": [],
   "source": [
    "torchMdl.load_state_dict(torch.load('../Weights/ResnetAllCls1.pt'))\n",
    "torchMdl.eval()\n",
    "# acc = 0\n",
    "# with torch.no_grad():\n",
    "#   for inputs, labels in testloader:\n",
    "#     inputs, labels = inputs.to(device),labels.to(device)\n",
    "#     logps = torchMdl.forward(inputs)    \n",
    "#     ps = torch.exp(logps)\n",
    "#     top_p, top_class = ps.topk(1, dim=1)\n",
    "#     equals = top_class == labels.view(*top_class.shape)\n",
    "#     acc += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "    \n",
    "# print(acc/len(testloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55cccb80-6492-4a1f-bcab-8855e6bf1a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_weights(backbone,full_transformer):\n",
    "  dic = backbone.state_dict() #take the weights from the trained backbone\n",
    "  for key in full_transformer.state_dict().keys(): #loop over the detr model\n",
    "    if 'backbone.0.body.' in key:\n",
    "      dic['backbone.0.body.'+key[16:]] = dic[key[16:]] #change the name in the trained backbone to include the necessary addition\n",
    "      del dic[key[16:]] #get rid of old names from the trained backbone\n",
    "  full_transformer.load_state_dict(dic, strict=False)\n",
    "  return full_transformer\n",
    "model = transfer_weights(torchMdl,model)\n",
    "\n",
    "# there are 500 weights in transformer backbone, and 628 in torch model, therefore all needed weights exist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5229fc5-9e2a-469f-a9c0-1d41ec04d59d",
   "metadata": {},
   "source": [
    "## Backbone: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3de70bc-48ce-4863-85fe-474db23c5ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "# steps = 0\n",
    "running_loss = 0\n",
    "print_every = 1\n",
    "train_losses, test_losses = [], []\n",
    "\n",
    "time_start = time.time()\n",
    "for epoch in range(epochs):\n",
    "    for idx, (inputs, labels) in enumerate(trainloader):\n",
    "      # steps += 1\n",
    "      inputs, labels = inputs.to(device), labels.to(device)\n",
    "      optimizer.zero_grad()\n",
    "      logps = torchMdl.forward(inputs)\n",
    "      # ps = torch.exp(logps.detach().cpu())\n",
    "      # _, top_class = ps.topk(1, dim=1)\n",
    "      # print(top_class)\n",
    "      # print(labels)\n",
    "      loss = criterion(logps, labels)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      running_loss += loss.item()\n",
    "      if idx%500==0:print('%d iteration is done'%(idx))\n",
    "      \n",
    "      \n",
    "    test_loss = 0\n",
    "    accuracy = 0\n",
    "    torchMdl.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device),labels.to(device)\n",
    "            logps = torchMdl.forward(inputs)\n",
    "            batch_loss = criterion(logps, labels)\n",
    "            test_loss += batch_loss.item()\n",
    "            \n",
    "            ps = torch.exp(logps)\n",
    "            top_p, top_class = ps.topk(1, dim=1)\n",
    "            \n",
    "            equals = top_class == labels.view(*top_class.shape)\n",
    "            accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "    train_losses.append(running_loss/len(trainloader))\n",
    "    test_losses.append(test_loss/len(testloader))                    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "          f\"Train loss: {running_loss/print_every:.3f}.. \"\n",
    "          f\"Test loss: {test_loss/len(testloader):.3f}.. \"\n",
    "          f\"Test accuracy: {accuracy/len(testloader):.3f}\")\n",
    "    running_loss = 0\n",
    "    torchMdl.train()\n",
    "\n",
    "time_tot = time.time()-time_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "09fe59a1-0a96-409e-bf81-164ecf50f5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nm = 'ResnetAllCls1.pt'\n",
    "torch.save(torchMdl.state_dict(), '../Weights/'+nm)\n",
    "writeIt = ['\\n', nm, 'was obtained with pretrained resnet101, with frozen weights except for fully-connected and 1 previous layer',\n",
    "           'with [2048 512] [512 2] as final layers, logsoftmax activation at the final layer',\n",
    "           'with nn.CrossEntropyLoss() and SGD() and a lr of 0.01, mom of 0.9',\n",
    "           'batch size is {0}'.format(batch_size),          \n",
    "           '{0} epochs excution time {1} using 1 GPU.'.format(epochs,time_tot/60),\n",
    "           'last reported test accuracy is {0}'.format(accuracy/len(testloader)),\n",
    "           'patch_size is 300x300, unlike the ones above that were 512x512',\n",
    "          'Data involves all datasets.']\n",
    "with open('../Weights/readme.txt', 'a') as f:\n",
    "    f.writelines('\\n'.join(writeIt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95afbfed-4b8c-4237-a0e2-1815cd11b9dc",
   "metadata": {},
   "source": [
    "## Step 0: get model and criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de8fa077-a1b2-41d4-bf14-63b339970475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_prepare(model=None,from_cell=True,\n",
    "                 frozen=1,device=None,DetectionFile='DetectionAll'):\n",
    "  parser = argparse.ArgumentParser(description='Detr finetuning',\n",
    "                                    formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "  parser.add_argument('--aux_loss',default = True,type=bool)\n",
    "  args, _ = parser.parse_known_args()\n",
    "  args.backbone='resnet101'\n",
    "  args.batch_size=8\n",
    "  args.bbox_loss_coef=5\n",
    "  args.clip_max_norm=0.1\n",
    "  args.coco_panoptic_path=None\n",
    "  args.coco_path='../../Datasets/'+DetectionFile\n",
    "  args.dataset_file='coco'\n",
    "  args.dec_layers=6\n",
    "  args.device='cuda'\n",
    "  args.dice_loss_coef=1\n",
    "  args.dilation=False\n",
    "  args.dim_feedforward=2048\n",
    "  args.dist_url='env://'\n",
    "  args.dropout=0.1\n",
    "  args.enc_layers=6\n",
    "  args.eos_coef=0.1\n",
    "  args.epochs=300\n",
    "  args.eval=False\n",
    "  args.frozen_weights=None\n",
    "  args.giou_loss_coef=2\n",
    "  args.hidden_dim=256\n",
    "  args.lr=0.0001\n",
    "  args.lr_backbone=1e-05\n",
    "  args.lr_drop=200\n",
    "  args.mask_loss_coef=1\n",
    "  args.masks=False\n",
    "  args.nheads=8\n",
    "  if from_cell:\n",
    "    args.num_queries=100 #setting number of queries to 10 only. If not used, then we can import the model as cell *** above\n",
    "  else:\n",
    "    args.num_queries=10\n",
    "    print('Using custom query number\\n')\n",
    "  args.num_workers=2\n",
    "  args.output_dir=''\n",
    "  args.position_embedding='sine'\n",
    "  args.pre_norm=False\n",
    "  args.remove_difficult=False\n",
    "  args.resume=''\n",
    "  args.seed=42\n",
    "  args.set_cost_bbox=6\n",
    "  args.set_cost_class=2\n",
    "  args.set_cost_giou=2\n",
    "  args.start_epoch=0\n",
    "  args.weight_decay=0.0001\n",
    "  args.world_size=1\n",
    "\n",
    "  # getting backbone weights, and transformer weights\n",
    "  if from_cell:\n",
    "    _, criterion, _ = build_model(args) \n",
    "    assert model != None\n",
    "  else:\n",
    "    model, criterion, _ = build_model(args)\n",
    "\n",
    "  # behind the scenes changed number of output nuerons to 1. Inside detr.py, \n",
    "  # This is needed if model is to be reutrened from the above line, incase \n",
    "  # args.num_classes above is set to a value other than 100. \n",
    "  model = transfer_weights(torchMdl,model) #model either comes from the above, or from cell ***\n",
    "  model = model.to(device)\n",
    "\n",
    "\n",
    "  # checkpoint = torch.hub.load_state_dict_from_url(\n",
    "  #             url=\"https://dl.fbaipublicfiles.com/detr/detr-r101-2c7b67e5.pth\", map_location=\"cpu\", check_hash=True\n",
    "  #         )\n",
    "  # model.load_state_dict(checkpoint[\"model\"],strict=False)\n",
    "\n",
    "  # The above is incomplete, but is needed (and should be completed) if args.num_classses is changed from 100.\n",
    "  # It goes with the above. The completion consists of popping out the query embedding, and the detection head\n",
    "  # from the state_dict. These must be set to trainable below. The block below only sets the detection heads\n",
    "  # to trainable, and not the query embedding. \n",
    "\n",
    "  # Freezing all except final detection heads\n",
    "  \n",
    "  for n,p in model.named_parameters():\n",
    "    if frozen==1:\n",
    "      if \"bbox_embed\" in n or \"class_embed\" in n or \"query\" in n:\n",
    "        p.requires_grad = True\n",
    "        # print(n+' is trainable')\n",
    "      else:\n",
    "        p.requires_grad = False\n",
    "    elif frozen==2:\n",
    "      if \"bbox_embed\" in n or \"class_embed\" in n or \"query\" in n or \"backbone\" in n:\n",
    "        p.requires_grad = True\n",
    "        # print(n+' is trainable')\n",
    "      else:\n",
    "        p.requires_grad = False\n",
    "    \n",
    "  return model, criterion, args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6030d9-2d63-4e95-9175-4d399ddbd4df",
   "metadata": {},
   "source": [
    "## Step 1: Main Loop, data_loader, and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac64879-70d3-45a1-9904-bb5c183bda62",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(8,880,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "586af3e0-d420-4cd8-99e3-5f995ffb13ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# misc functions\n",
    "\n",
    "def get_stats2(pred_bb,GT_bb,r = 80,sz=800,raw=False):\n",
    "  pred_bb = pred_bb.detach().cpu().numpy()\n",
    "  if not raw:\n",
    "    GT_bb = GT_bb.detach().cpu().numpy() * numpy.array([800, 800, 800, 800])\n",
    "  \n",
    "  TP, TN, FP, FN, N, P = 0,0,0,0, 0, 0\n",
    "  total_num = (sz/r) * (sz/r)\n",
    "  for row in range(r,sz+r,r):\n",
    "    for col in range(r,sz+r,r):\n",
    "      arr_GT = numpy.prod(numpy.array([col-r<GT_bb[:,0],\n",
    "                          GT_bb[:,0]<=col,\n",
    "                          GT_bb[:,1]<=row,\n",
    "                          GT_bb[:,1]>row-r]),0)\n",
    "      \n",
    "      arr_pred = numpy.prod(numpy.array([col-r<pred_bb[:,0],\n",
    "                          pred_bb[:,0]<=col,\n",
    "                          pred_bb[:,1]<=row,\n",
    "                          pred_bb[:,1]>row-r]),0)\n",
    "           \n",
    "      if sum(arr_GT) == 0:\n",
    "        N += 1\n",
    "        if sum(arr_pred) == 0:\n",
    "          TN += 1 \n",
    "        else:\n",
    "          FP += 1\n",
    "      else:\n",
    "        P += 1\n",
    "        if sum(arr_pred) == 0:\n",
    "          FN += 1\n",
    "        else:\n",
    "          TP += 1\n",
    "  return TP, TN, FP, FN, P, N\n",
    "\n",
    "def get_stats(pred_bb,GT_bb,threshold = 6):\n",
    "  \n",
    "  pred_bb = pred_bb.detach().cpu().numpy()\n",
    "  GT_bb = GT_bb.detach().cpu().numpy() * numpy.array([800, 800, 800, 800])\n",
    "  dist_mat = dist(pred_bb[:,:2],GT_bb[:,:2])\n",
    "  \n",
    "  a, b = dist_mat.shape\n",
    "  TP, FP, FN = 0, 0, 0\n",
    "  min_pred = numpy.min(dist_mat,1)\n",
    "  min_gt = numpy.min(dist_mat,0)\n",
    "  TP = sum(min_pred<threshold)\n",
    "  FP = a - TP\n",
    "  FN = sum(min_gt>threshold)\n",
    "    \n",
    "  return TP/a, FP/a, FN/b #TN\n",
    "\n",
    "def reduce_bb(wanted_bb,choice='dist',th=50):\n",
    "  \n",
    "  boxs = wanted_bb.cpu().detach().numpy() * numpy.array([800, 800, 800, 800])\n",
    "  clustering = clst(n_clusters=None,distance_threshold=th).fit_predict(boxs[:,:2])\n",
    "  centroids = []\n",
    "  for cluster in numpy.unique(clustering):\n",
    "    clustering_idx = numpy.where(clustering==cluster)[0]\n",
    "    centroids += [sum(boxs[clustering_idx],0)/len(clustering_idx)] # plus ignored average width and height info\n",
    "  \n",
    "  return torch.Tensor(centroids)\n",
    "\n",
    "def get_drw_from_tnsr(x,black_bg=False):\n",
    "  if not black_bg:\n",
    "    for i in range(3):\n",
    "      x[:,:,i] -= min(x[:,:,i].flatten())\n",
    "      x[:,:,i] /= max(x[:,:,i].flatten())\n",
    "    x = (x*255).round().astype(numpy.uint8)\n",
    "  else:\n",
    "    x = numpy.zeros(x.shape,dtype=numpy.uint8)\n",
    "  \n",
    "  im = Image.fromarray(x)\n",
    "  im2 = im.copy()\n",
    "  drw = ImageDraw.Draw(im2)\n",
    "  return drw, im2\n",
    "\n",
    "def draw_bb(boxs,drw,im2,choice='rect',raw=False,outline='black',dot_size=7,im_size=800):  \n",
    "  for box in boxs:\n",
    "    if not raw:\n",
    "      box = box.cpu() * torch.Tensor([im_size, im_size, im_size, im_size])\n",
    "    x, y, w, h = box\n",
    "    x0, x1 = x-w//2, x+w//2\n",
    "    y0, y1 = y-h//2, y+h//2\n",
    "    if choice == 'rect':\n",
    "      drw.rectangle([x0, y0, x1, y1],outline=outline)\n",
    "    elif choice == 'point':\n",
    "      # drw.rectangle([x-dot_size, y-dot_size, x+dot_size, y+dot_size],outline=outline,fill=outline)\n",
    "      drw.ellipse([x-dot_size, y-dot_size, x+dot_size, y+dot_size],outline=outline,fill=outline)\n",
    "    elif choice == 'cross':\n",
    "      drw.line([(x-1.6*dot_size, y-1.6*dot_size),(x+1.6*dot_size, y+1.6*dot_size)], fill=outline,width=5)\n",
    "      drw.line([(x-1.6*dot_size, y+1.6*dot_size),(x+1.6*dot_size, y-1.6*dot_size)], fill=outline,width=5)\n",
    "    \n",
    "  return drw\n",
    "\n",
    "def draw_bb_crc(boxs,drw,im2,choice='rect',raw=False,outline='black',dot_size=3,im_size=800):  \n",
    "  for box in boxs:\n",
    "    if not raw:\n",
    "      box = box.cpu() * torch.Tensor([im_size, im_size])\n",
    "    x0, y0 = box  \n",
    "    drw.rectangle([x0-dot_size, y0-dot_size, x0+dot_size, y0+dot_size],outline=outline,fill=outline)\n",
    "  return drw\n",
    "\n",
    "def IoU(bb1,bb2):\n",
    "  x1a, y1a, x2a, y2a = bb1\n",
    "  x1b, y1b, x2b, y2b = bb2\n",
    "  xA = max(x1a,x1b)\n",
    "  yA = max(y1a,y1b)\n",
    "  xB = max(x2a,x2b)\n",
    "  yB = max(y2a,y2b)\n",
    "\n",
    "  interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "  boxAArea = (x2a - x1a + 1) * (y2a - y1a + 1)\n",
    "  boxBArea = (x2b - x1b + 1) * (y2b - y1b + 1)\n",
    "  iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "  return iou\n",
    "\n",
    "def cls_acc(pred,gt):\n",
    "  return float(sum(cls_acc==gt))/float(len(gt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2cc035d8-e742-47bc-890e-b52159121139",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nofelvsap/anaconda3/envs/nucdetr/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/nofelvsap/anaconda3/envs/nucdetr/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.29s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.07s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "DetectionFile = 'consep/ForDetection'\n",
    "_, _, args = base_prepare(model,from_cell=True,device=device,DetectionFile=DetectionFile)\n",
    "dataset_train = build_dataset(image_set='train', args=args)\n",
    "dataset_val = build_dataset(image_set='val', args=args)\n",
    "sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
    "sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "batch_sampler_train = torch.utils.data.BatchSampler(\n",
    "sampler_train, args.batch_size, drop_last=True)\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, batch_sampler=batch_sampler_train,\n",
    "                                  collate_fn=utils.collate_fn, num_workers=args.num_workers)\n",
    "data_loader_val = DataLoader(dataset_val, args.batch_size, sampler=sampler_val,\n",
    "                                drop_last=False, collate_fn=utils.collate_fn, num_workers=args.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2804f1cd-0e16-4322-a8d7-cbc69d13cedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_train.dataset.coco.imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87383acd-97d7-41dc-9e96-e90e975890d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fr = 0 #the freezing mode\n",
    "idx = 34 #the experimnetation index\n",
    "cont = True #to continue (True) or startover (False)\n",
    "\n",
    "model = get_pretrained_detr()\n",
    "model, criterion, args = base_prepare(model,from_cell=True,frozen=fr,device=device)\n",
    "# base\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr,\n",
    "                                  weight_decay=args.weight_decay)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, args.lr_drop)\n",
    "# end of base\n",
    "\n",
    "if cont:\n",
    "  #for model\n",
    "  checkpoint = torch.load('../Weights/DetrMdl'+str(idx-1)+'.pth')\n",
    "  model.load_state_dict(checkpoint['state_dict'])\n",
    "  optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "  criterion.load_state_dict(checkpoint['criterion'])\n",
    "  lr_scheduler.load_state_dict(checkpoint['lr'])\n",
    "  #for stats\n",
    "  # loaded = sio.loadmat('/content/drive/My Drive/Colab/CRC/weights/DetrMdl'+str(idx-1)+'.mat')\n",
    "  # old_loss_ce_avg_ts = list(loaded['loss_ce_avg_ts'].flatten())\n",
    "  # old_loss_bb_avg_ts = list(loaded['loss_bb_avg_ts'].flatten())\n",
    "  # old_loss_giou_avg_ts = list(loaded['loss_giou_avg_ts'].flatten())\n",
    "  # old_loss_ce_avg_tr = list(loaded['loss_ce_avg_tr'].flatten())\n",
    "  # old_loss_bb_avg_tr = list(loaded['loss_bb_avg_tr'].flatten())\n",
    "  # old_loss_giou_avg_tr = list(loaded['loss_giou_avg_tr'].flatten())\n",
    "  starting_epoch = checkpoint['epoch']\n",
    "  \n",
    "else:    \n",
    "  starting_epoch = 0\n",
    "  # old_loss_ce_avg_tr = []\n",
    "  # old_loss_bb_avg_tr = []\n",
    "  # old_loss_giou_avg_tr = []\n",
    "  # old_loss_ce_avg_ts = []\n",
    "  # old_loss_bb_avg_ts = []\n",
    "  # old_loss_giou_avg_ts = []\n",
    "\n",
    "loss_ce_avg_tr = []\n",
    "loss_bb_avg_tr = []\n",
    "loss_giou_avg_tr = []\n",
    "loss_ce_avg_ts = []\n",
    "loss_bb_avg_ts = []\n",
    "loss_giou_avg_ts = []\n",
    "epochs = starting_epoch + 400\n",
    "start = time.time()\n",
    "save_every = 200\n",
    "for epoch in range(starting_epoch,epochs):\n",
    "  loss_bb_tr = 0\n",
    "  loss_ce_tr = 0\n",
    "  loss_giou_tr = 0\n",
    "  loss_bb_ts = 0\n",
    "  loss_ce_ts = 0\n",
    "  loss_giou_ts = 0\n",
    "  model.train()\n",
    "  criterion.train()\n",
    "  metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "  metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "  metric_logger.add_meter('class_error', utils.SmoothedValue(window_size=1, fmt='{value:.2f}'))\n",
    "  header_tr = 'Epoch: [{}]'.format(epoch)\n",
    "  print_freq = args.batch_size*5\n",
    "\n",
    "  ## Training Block\n",
    "  for samples, targets in metric_logger.log_every(data_loader_train, print_freq, header_tr):\n",
    "    \n",
    "    samples = samples.to(device)\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "    outputs = model(samples)\n",
    "    loss_dict = criterion(outputs, targets)\n",
    "    weight_dict = criterion.weight_dict\n",
    "    losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
    "\n",
    "    # reduce losses over all GPUs for logging purposes\n",
    "    loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
    "    loss_dict_reduced_unscaled = {f'{k}_unscaled': v\n",
    "                                  for k, v in loss_dict_reduced.items()}\n",
    "    loss_dict_reduced_scaled = {k: v * weight_dict[k]\n",
    "                                for k, v in loss_dict_reduced.items() if k in weight_dict}\n",
    "    losses_reduced_scaled = sum(loss_dict_reduced_scaled.values())\n",
    "\n",
    "    loss_value = losses_reduced_scaled.item()\n",
    "\n",
    "    if not math.isfinite(loss_value):\n",
    "        print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "        print(loss_dict_reduced)\n",
    "        sys.exit(1)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    losses.backward()    \n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip_max_norm)\n",
    "    optimizer.step()\n",
    "    \n",
    "    metric_logger.update(loss=loss_value, **loss_dict_reduced_scaled)\n",
    "    metric_logger.update(class_error=loss_dict_reduced['class_error'])\n",
    "    metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "    loss_ce_tr += loss_dict_reduced_scaled['loss_ce'].cpu().detach().numpy()\n",
    "    loss_bb_tr += loss_dict_reduced_scaled['loss_bbox'].cpu().detach().numpy()\n",
    "    loss_giou_tr += loss_dict_reduced_scaled['loss_giou'].cpu().detach().numpy()\n",
    "    \n",
    "    \n",
    "  ## End of Training Block\n",
    "  # gather the stats from all processes  \n",
    "  metric_logger.synchronize_between_processes()\n",
    "  print(\"Averaged stats:\", metric_logger)\n",
    "  train_stats = {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n",
    "  lr_scheduler.step() \n",
    "  loss_ce_avg_tr += [loss_ce_tr/len(data_loader_train)]\n",
    "  loss_bb_avg_tr += [loss_bb_tr/len(data_loader_train)]\n",
    "  loss_giou_avg_tr += [loss_giou_tr/len(data_loader_train)]\n",
    "  ## Testing Block\n",
    "  with torch.no_grad():\n",
    "    model.eval()  \n",
    "    criterion.eval()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('class_error', utils.SmoothedValue(window_size=1, fmt='{value:.2f}'))\n",
    "    header_ts = 'Test:'\n",
    "    for samples_ts, targets_ts in metric_logger.log_every(data_loader_val, 40, header_ts):\n",
    "      samples_ts = samples_ts.to(device)\n",
    "      targets_ts = [{k: v.to(device) for k, v in t.items()} for t in targets_ts]\n",
    "      outputs_ts = model(samples_ts)\n",
    "      loss_dict_ts = criterion(outputs_ts, targets_ts)\n",
    "      weight_dict_ts = criterion.weight_dict\n",
    "\n",
    "      # reduce losses over all GPUs for logging purposes\n",
    "      loss_dict_reduced = utils.reduce_dict(loss_dict_ts)\n",
    "      loss_dict_reduced_scaled = {k: v * weight_dict[k]\n",
    "                                  for k, v in loss_dict_reduced.items() if k in weight_dict}\n",
    "      loss_dict_reduced_unscaled = {f'{k}_unscaled': v\n",
    "                                    for k, v in loss_dict_reduced.items()}\n",
    "      metric_logger.update(loss=sum(loss_dict_reduced_scaled.values()),\n",
    "                            **loss_dict_reduced_scaled)\n",
    "      metric_logger.update(class_error=loss_dict_reduced['class_error'])\n",
    "      loss_ce_ts += loss_dict_reduced_scaled['loss_ce'].cpu().detach().numpy()\n",
    "      loss_bb_ts += loss_dict_reduced_scaled['loss_bbox'].cpu().detach().numpy()\n",
    "      loss_giou_ts += loss_dict_reduced_scaled['loss_giou'].cpu().detach().numpy()\n",
    "    loss_ce_avg_ts += [loss_ce_ts/len(data_loader_val)]\n",
    "    loss_bb_avg_ts += [loss_bb_ts/len(data_loader_val)]\n",
    "    loss_giou_avg_ts += [loss_giou_ts/len(data_loader_val)]\n",
    "    ## End of Testing Block\n",
    "    \n",
    "\n",
    "    # loss_ce_avg_tr += old_loss_ce_avg_tr\n",
    "    # loss_bb_avg_tr += old_loss_bb_avg_tr\n",
    "    # loss_giou_avg_tr += old_loss_giou_avg_tr\n",
    "    # loss_ce_avg_ts += old_loss_ce_avg_ts\n",
    "    # loss_bb_avg_ts += old_loss_bb_avg_ts\n",
    "    # loss_giou_avg_ts += old_loss_giou_avg_ts\n",
    "    if (epoch+1)%save_every == 0:\n",
    "        time_tot = time.time()-start\n",
    "        nm = 'DetrMdl'+str(idx)+'.pth'\n",
    "        # torch.save(model.state_dict(),'../Weights/'+nm)\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'criterion': criterion.state_dict(),\n",
    "            'lr':lr_scheduler.state_dict()\n",
    "        }\n",
    "        torch.save(checkpoint,'../Weights/'+nm)\n",
    "        writeIt = ['\\n', nm, 'is the full Detr model, with the original settings',\n",
    "                  'batch size is {0}'.format(args.batch_size),\n",
    "                  'number of queries is {0}'.format(args.num_queries),\n",
    "                  'backbone is only initialized with ImageNet', \n",
    "                    'frozen mode is = {0}'.format(fr), \n",
    "                    'augmentation over training data is removed',     \n",
    "                  '{0} epochs excution time {1}.'.format(epochs,time_tot/60),\n",
    "                   'Using the new SegToBB protocols',\n",
    "                   'The classification cost is increased to 2 (was 1 for all the above)',\n",
    "                   'The bounding box cost is increased to 6, (was 5 for all the above)',\n",
    "                   'Training of backbone is on all data, training of detection pipeline is only using PanNuke data']\n",
    "        with open('../Weights/readme.txt', 'a') as f:\n",
    "            f.writelines('\\n'.join(writeIt))\n",
    "        sio.savemat('../Weights/'+nm[:-4]+'.mat',{'loss_ce_avg_ts':loss_ce_avg_ts,\n",
    "                                                                                'loss_bb_avg_ts':loss_bb_avg_ts,\n",
    "                                                                                'loss_giou_avg_ts':loss_giou_avg_ts,\n",
    "                                                                                'loss_ce_avg_tr':loss_ce_avg_tr,\n",
    "                                                                                'loss_bb_avg_tr':loss_bb_avg_tr,\n",
    "                                                                                'loss_giou_avg_tr':loss_giou_avg_tr})\n",
    "        idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baacb245-144a-484a-abb6-64164b93d5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_pretrained_detr()\n",
    "checkpoint = torch.load('../Weights/DetrMdl35.pth')\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3ee578-1cd6-4181-b783-0f09ce7af33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ce_ts,loss_bb_ts,loss_giou_ts,loss_ce_tr,loss_bb_tr,loss_giou_tr = [],[],[],[],[],[]\n",
    "for i in range(10,13):\n",
    "  loaded = sio.loadmat('../Weights/DetrMdl'+str(i)+'.mat')\n",
    "  loss_ce_avg_ts = list(loaded['loss_ce_avg_ts'].flatten())\n",
    "  loss_bb_avg_ts = list(loaded['loss_bb_avg_ts'].flatten())\n",
    "  loss_giou_avg_ts = list(loaded['loss_giou_avg_ts'].flatten())\n",
    "  loss_ce_avg_tr = list(loaded['loss_ce_avg_tr'].flatten())\n",
    "  loss_bb_avg_tr = list(loaded['loss_bb_avg_tr'].flatten())\n",
    "  loss_giou_avg_tr = list(loaded['loss_giou_avg_tr'].flatten())\n",
    "  loss_ce_ts += loss_ce_avg_ts\n",
    "  loss_bb_ts += loss_bb_avg_ts\n",
    "  loss_giou_ts += loss_giou_avg_ts\n",
    "  loss_ce_tr += loss_ce_avg_tr\n",
    "  loss_bb_tr += loss_bb_avg_tr\n",
    "  loss_giou_tr += loss_giou_avg_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b25de91d-88ca-4dba-b9cc-e4862f3bf7b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_ce_avg_tr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4695/763153081.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_ce_avg_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_ce_avg_ts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"class error tr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"class error ts\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss_ce_avg_tr' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(loss_ce_avg_tr)\n",
    "plt.plot(loss_ce_avg_ts)\n",
    "plt.legend([\"class error tr\", \"class error ts\"])\n",
    "plt.figure()\n",
    "plt.plot(loss_bb_avg_tr)\n",
    "plt.plot(loss_bb_avg_ts)\n",
    "plt.legend([\"bb error tr\", \"bb error ts\"])\n",
    "plt.figure()\n",
    "plt.plot(loss_giou_avg_tr)\n",
    "plt.plot(loss_giou_avg_ts)\n",
    "plt.legend([\"giou error tr\", \"giou error ts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c58468-da2a-43b8-ab19-da92d3d14391",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9bcd48-8519-411d-a04e-8af425e00033",
   "metadata": {
    "tags": []
   },
   "source": [
    "## To get the images names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42ade86-8202-40bc-b8bf-23ee6812def3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_val.dataset.coco.imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0946cdc2-81e3-4521-bffb-4cab7553fce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "description: Consep Detection Dataset\n",
      "url: www.ku.ac.ae\n",
      "version: 1.0\n",
      "year: 2021\n",
      "contributor: Ahmad Obeid, member of Dr. Naoufel's research group at KU\n",
      "date_created: 2021\n"
     ]
    }
   ],
   "source": [
    "data_loader_val.dataset.coco.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c71ace-287b-4b54-87a3-a8df29d8bf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wi = 0\n",
    "with torch.no_grad():\n",
    "    model.eval()  \n",
    "    criterion.eval()\n",
    "    \n",
    "    for samples_ts, targets_ts in data_loader_train:           \n",
    "      samples_ts = samples_ts.to(device)\n",
    "      targets_ts = [{k: v.to(device) for k, v in t.items()} for t in targets_ts]\n",
    "      outputs_ts = model(samples_ts)\n",
    "      for idx, (output_cls, output_bb)  in enumerate(zip(outputs_ts['pred_logits'],outputs_ts['pred_boxes'])):\n",
    "        xx = samples_ts.tensors[idx].permute(1,2,0).cpu().detach().numpy()        \n",
    "        drw, im2 = get_drw_from_tnsr(xx)\n",
    "        wanted_idx = numpy.where(output_cls.argmax(-1).cpu().detach().numpy()==0)[0]\n",
    "        wanted_bb = output_bb[wanted_idx]\n",
    "        drw = draw_bb(wanted_bb,drw,im2)       \n",
    "        pdb.set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e5af69-7d3a-4f1b-aec8-ccf7be714040",
   "metadata": {},
   "outputs": [],
   "source": [
    "im2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5deea9-9e4d-458e-b759-0bfef5f1dbdb",
   "metadata": {},
   "source": [
    "## Testing on individual datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6335da69-751a-4e81-af19-63f322bb75aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_train.dataset.coco.imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "44dfafed-2291-498e-9175-2eb53fef2335",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nofelvsap/anaconda3/envs/nucdetr/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/nofelvsap/anaconda3/envs/nucdetr/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.20s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.07s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "DetectionFile = 'consep/ForDetection'\n",
    "_, _, args = base_prepare(model,from_cell=True,device=device,DetectionFile=DetectionFile)\n",
    "dataset_train = build_dataset(image_set='train', args=args)\n",
    "dataset_val = build_dataset(image_set='val', args=args)\n",
    "sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
    "sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "batch_sampler_train = torch.utils.data.BatchSampler(\n",
    "sampler_train, args.batch_size, drop_last=True)\n",
    "\n",
    "# individual_loader_train = DataLoader(dataset_train, batch_sampler=batch_sampler_train,\n",
    "                                  # collate_fn=utils.collate_fn, num_workers=args.num_workers)\n",
    "individual_loader_val = DataLoader(dataset_val, args.batch_size, sampler=sampler_val,\n",
    "                                drop_last=False, collate_fn=utils.collate_fn, num_workers=args.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e47b677-e207-44cd-82c2-f4743e323d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantitative detection evaluation\n",
    "from sklearn.metrics.pairwise import euclidean_distances as dist\n",
    "from sklearn.cluster import AgglomerativeClustering as clst\n",
    "thre = [1]\n",
    "for th in thre:\n",
    "  lengths = []\n",
    "  with torch.no_grad():\n",
    "      model.eval()  \n",
    "      criterion.eval()\n",
    "      wanted_bb = []\n",
    "      TP, TN, FP, FN, P, N = [], [], [], [], [], []\n",
    "      for idx1, (samples_ts, targets_ts) in enumerate(data_loader_val):\n",
    "        samples_ts = samples_ts.to(device)\n",
    "        targets_ts = [{k: v.to(device) for k, v in t.items()} for t in targets_ts]\n",
    "        outputs_ts = model(samples_ts)\n",
    "        \n",
    "        for idx, (output_cls, output_bb)  in enumerate(zip(outputs_ts['pred_logits'],outputs_ts['pred_boxes'])):\n",
    "          # print('GT has {} bbs'.format(len(targets_ts[idx]['boxes'])))\n",
    "          GT_bb = targets_ts[idx]['boxes']\n",
    "          xx = samples_ts.tensors[idx].permute(1,2,0).cpu().detach().numpy()\n",
    "          \n",
    "          wanted_idx = numpy.where(output_cls.argmax(-1).cpu().detach().numpy()==0)[0]\n",
    "          wanted_bb = output_bb[wanted_idx]   \n",
    "          if len(wanted_bb) == 0:\n",
    "            continue\n",
    "          elif len(wanted_bb) == 1:\n",
    "            reduced_bb = wanted_bb.cpu() * torch.Tensor([800, 800,800, 800])\n",
    "          else:\n",
    "            reduced_bb = reduce_bb(wanted_bb,th=th)\n",
    "          \n",
    "#           drw, im2 = get_drw_from_tnsr(xx,black_bg=True)\n",
    "#           drw = draw_bb(GT_bb,drw,im2,choice='point',raw=False,outline='blue',dot_size=18)\n",
    "#           drw = draw_bb(reduced_bb,drw,im2,choice='point',raw=True,outline='red',dot_size=8)\n",
    "#           # drw = draw_bb(GT_bb,drw,im2,choice='rect',raw=False,outline='blue')\n",
    "\n",
    "# #           pdb.set_trace()\n",
    "\n",
    "          stats = get_stats2(reduced_bb,GT_bb,r=80)\n",
    "          TP += [stats[0]]\n",
    "          TN += [stats[1]]\n",
    "          FP += [stats[2]]\n",
    "          FN += [stats[3]]\n",
    "          P += [stats[4]]\n",
    "          N += [stats[5]]\n",
    "          # pdb.set_trace()\n",
    "          \n",
    "        \n",
    "  TP_all,TN_all,FP_all,FN_all,P_all,N_all = sum(TP),sum(TN),sum(FP),sum(FN),sum(P),sum(N)   \n",
    "  prec, recall = TP_all/(TP_all+FP_all), TP_all/(TP_all+FN_all)\n",
    "  F = 2*prec*recall/(prec+recall)\n",
    "  print('Preceision is: {}.'.format(prec))\n",
    "  print('Recall is: {}.'.format(recall))\n",
    "  print('F-score is: {}.'.format(F))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ce5ee77a-6f83-4dfa-87e9-7c9be3d1190a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 800, 800])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_ts.tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "14740d0d-af85-47b8-bc33-304a779e1d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAyAAAAMgCAIAAABUEpE/AABCWElEQVR4nO3d63KjPLoGUHuq7/+WPT/cnS9xbJDQ6ZW0Vk3t2ZN2HNCJBwHi/ridu99SPnW73e5pHwMWkzhEDDT16FSxeKcuB5jJ/6p+W/xBFqhOx2+qbvGqLOjknjw3ZRIL+O18ZHj8HBaSB5O6Jh2aGpXVpKUBM0mdwXrokECmx+3+e+h4+0OAxdS9RHgz/ww7OervxylKxkrQbjg1UENz1QMWsInr6Sr9MxtrnYFkLGjrz+gNAFaTnpwet/ugW7KmF+PONuAjAQu4YK7D+XRTZdmzg88fZsasx4QlA9NwiRCoKffCnwuFWdzZBrNIDVjmnwHGcmcbTCQpYElXwDdzDQjTBY73xZt1Z1vh3wLKuUQIAFDZ/+63x8EE1fG/AsQ23fTVe+5sg+n8ncF6m6IuRSu9GraWO260PIUzHAHD/LdMg5kqAIAq3IMFVJZ+tmb6ClhV3YBlRANut7TkJF0BCzODBTRxnJ+apav7kukq0p1tQJKKAWvBQQ14J7Wzv30MueWzyWuMQj33Yo0Sg4julRaa00thK9EmSBYbgorWGs3Jr4uVGwRSZQZLFwUaOR5e7qteE3xr9J1tQIbyGaxdhjbgpw4H8p2Hl4/FezCPlZmudi5eaO7P+UeO6J8AXT1T1EvMMnEF0ZTMYElXsLmmB3UjjOKFiV2+B0vnBNqNA0aYm+KFqf0vv6dtdEspAMAFL1HpYEZaqALeqn4ly2jzneKFKelpQLmKIcCg9JvihfnobEAtc6z58rKV8wyCcxQv8KTLARWFfip5/nsgQhcv8J1eB1Q3arnLowR1uk3zjIZWE4UJ6HtAO33mjM4DR/Jb/OYy/5QcrEsnBOaVNJeTmK6ejIlAFVVe9gzQn5fDAHE5WwNmlJqusqavngyLMM46F74LX/YM0J+5K1hMSqf++swcSUvAAuYiXcFKLvTo569Ej1nuwQImIl3BSkp6dPTRQMACZhF9PAVylPfo0GOCgAVM4eJIes/8xehXHWARtbJR3IwlYAEAPdVNRUEzloAFxFc0gKZPYpm+gvZa5KGIGUvAAtaXkrFyLyYC+dr1snD9V8ACtnCcn6QraK91L4vVi82IA8FVHjRf1nb/Fa2MitBIhwAUqP8G2hSAdzqflRoVoYVuHTlKF7aSOwAwk7N56BAELACY1DqvRk709vXtzx9Gi1lrVgCwEJcI4UVWp4jQpOv04rfp6rt/GSvCLpvBAoBpHCWVDxfOAmWOEqfp6vmZOPNY05f4irab8v1AOfBkBguePvaFg/DxLXAMbNulvTglXX0J0ofNYMWR0v4i9JPWlAPAb1fS1e3HpM7DmNmTso7gcrRfrPqUA2+ZwYKL6erL6Hmsol6cNX31FKEbW8l9uJJmF+VKcw3KgU96DpURhmV4UZqufn7SgNmJgDVWeUNfo6soB4C3dnk18noErIFqNfHZu4pyAHgr45nBhO+6//hfNCZgjVK3cc/bVZQDKfpcuXN9kFD2ejXyegSsIUz5PikHgB0Unb3kLm0V5FRJwOqvXQKYK1soB7K0HjODjMnw1GccM1o2ZB2szjpM+U5xnLiwGHHu909RDmS5N+tBWguEdr89khekiCLOlmwi7/BwKWpMUafvdyRtMeJEU5QDF1TPWJoKASW188z1zd9+Z8/233w991Cd2SXCnjLa1uN2/92S3v6w5K8Mkp2uTv81/a8wv7pDaKgBGZ56Dl8zDZXHZ9px3kL4JGBFVDtqTCDxLZ4dtoQZ1GoJWhRzS48UMcJHhR53vz1+78u/H8bq0bG2ZnU1p3wPe0vwan3d8hqz3B8+y8oKDxibNI95B4qdFd1M8tvZsNm5JexyJ6UZrG4qX1A//GSEM5VPTH1TS8l4Gm4sru3x7z+Fn2ECc104a9P7IvZoTxEy2IXFiOONF4zybDxZ7SHiQFzVtadubxuUzMqeo2KNR7D7qPtEcNCmK2AFImrAJSlvsQ06BNdW+NL0TUppWVMdEWplrLiNVsAClhF3qO2iykvTNy9DeirPWKGbq4AFsICTA1XyxSMZi55KMlb0hipgAcwu49UI33/4IWbJWPS07J2UniJksPy3eE50kwF0kJ2uEv5VLxurZ4AIElbu//5T+JlAzGAFkv6upa/Pt9sYYAbX09XXZ8xjEck6rc4MFuPNthgxBFGars4+qbvBdQJWN+Xrs6d/MvIZwPttS9lx73sGttFn+DJINiRghdMmakxgtsWIYaw601dnn9fv4CIBK6Jto8bhWzwBttJ6esn0VVvKt7OiV3im5Ywp6rRDYJqiHOCymjNYt6PhRVcaa5dXI6/HU4ShmbkB3un80nQH44Hqvrbv6ztpziXCzkz5PikHgER1BzTDYycCVn/tGvdc3UY5QCvW711OrWHN8NiPgDVEiyY+Y7dRDgCJygc3w2NXAtYopnyflAM0Yf3eFZUMcYbH3gSsgUz5PikHaGLbRfWWduFlfDO9v28lCn24wtFtmRpUDpAuo78cLNmQnK70r7AmfaXHFlRABJezxWLVpxwgUXZnubSo3rePA5l0mziMd0/KAU51vq6no0E23SYgU75PygEOdMtYuhtcoecAzEjAgtDGvirHFAVcoOMARDdkOHaTDVyg4/DCS9Mhrs6dx2NicIGOw1sCFsTVc6HRkrHAUnhsS8fhEy9Nh7i6Bazygd6hgg3pOBzz0nQIqk8XqjjE6/Ps46jjZK4bqeOsrXqM1mCgVIdelNrzkw8Yej47+Nhxrr75RMdZm/NYiKV1R0rq8/kHDP2ftV1JV08y1saqZCyNBOpoeg9Wabr6/K9uK2Fh19PV2Wd0nLWVZyPpCqppF7AqpKvDzzhUsKSidJXwSR1nbSUJSbqCmnou0/Cq0gEDgC/3/Kh04VeAE40CVp+zZOfiLKbC9FXC53WcHdwTYlPKZ4CLhr2L8MIB4+xBdABeyE8wxshLhAAAS2oRsHrOM5nTYhk6DsA6zGABAFQmYMEEcm9AdMMiwFjDApYDBgCwKjNYMIf0cwxnIwDDjQxYDhiQJaUj6CwAEQyewXLAgCzH3UFnAQhi/CVCBwzIcr89fveLtz8EYJQWi/xeHOVf1nZPPlpYp5g1dI5HOg5AQ40G2W6HCgcJVqLjAHRwMNhWGx6HvYsQAKCjlJPYr8+UJi0BCwBY24XrA89fuR6z2l0p6HCxw2UO1qPjLKPHNQggQeG4erHDmsECqKjrNQjgTPlZ6+NaV23avZueixuYWJWOM6nLFadSoJGKw2l2P226Dla7UcN4xMJ0nBmVjOMWMIMW6vas7G9rvdBoiwHdQYLl6ThzqXINAqioRZ/K+84OK7nXHdYdJNiEjjOLWuO4jAW1tOtNGd/c51U5tQZ3Bwm2ouPEN/gaBPBL636U+v3d3kVYPsQ7SLAhHSey8dcggLB6vuy5ZKB3kGBbOk5MIa5BAD/16T5Jf2XI+Ju1/44Q8KTjxJFaF1ffYX9Tg3BJoDe6ju3DVjqGC3Sc4c4H8cfnukiLWaoSLhCwAGZVlK6eZCxooPPl9ZMe2vMeLID1naarxM8AUxOwAKpJT04yFqxNwAJI1/MahMcJYWICFkAduZNSJrFgYQIWAEBlAhYAQGUCFgBAZX9GbwAAENTLoxZuG0ynrADSnTzZl3XfesJyo4ZohpnzlRF5PTTn7VUvzsvADBYA8MNx7niEzljvvT35ef6wIGYdcQ8WQDXpI3WjMR3KpTTNuZrv8dRyowVTBCyAdClveD0/9HgXIWGlJ6eQGevjNNWxzIyV9GEBC6Cy4/xk7gq6Gfj2KgELoL777fE7SL39IcSR2zpDtubWU7+p328KGiBXh8OKwZkBLrTsqC31cbs0KXV2CpTxhWawAIDFtAh+ed8pYAHkinINAvisbj/K/jYBC+CCdhlIuoJorvRKAQvgmvHXIKCu3Pa3R3u9uJcCFsBlg69BAAfy8+LvO9yv90oBC6BErVQkXRFCekNcvcneC3dx9fIB6KFw4QZDMbGs9B7yxM75b/qq2p5NVEQAkV3OWMZhIjpo0NM12SF5cbpSAogsK2YZgYnupUHP22T758V5ywogspXO/2ERy+RFAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACA6D69SfrTe+C9eRq29WlYuHUfGeJsyVO07alisZ1abHeail9W0bbw/fbcUz6U8IvAqtKHhVvjkSHOljxF254qFtupxXanqfhlFW0LT7bnawuytvv3rwPruTYs3BqMDHG25Cna9lSx2E4ttjtNxS+raFuYtD3/S//o5b8BTKikd9cdGeJsSfl3hh0zF9upxXanqfhlFW0LU7/zXunPbxj5YWFxhoU4W/IUbXuqWGynFtudpuKXVbQtzNie//X/k0Bstbpz+ffE2ZKY31PFYju12O40Fb+som1h3vfUCljZfxgIadWre3HSXotvu2yxnVpsd5qKX1bRtjD7GyoGLGB2cUbJOFtS/rs9v3P4BgzcqcV2p6n4ZRVtC6/8bq17sL5/ITCjpseSrJEhzpY8RdueKhbbqYzdefzcvPv57y52XItf9dG28OL2VA9Yt+XaIuygw5l64sgQZ0ueom1PFYvtVOruPD5v1VnMWua4Fr/qo23h9e358/krczM+AAR1kK6e/+owR11vZrAKMv73rwVmkdSva5x0nY4M3Y5wcabTniLO9xT/mZcG0+zvJH0o6e8ftuoFjmvx23O0LSzantcZLBkfePF2WHj+cOEB4XG7fR+FF97T6j40mNtt2AtWUv+sYxwV/XiKMKUVprdUYAGnJ13dtqSbx7vz1sftvuTOVnfWYGAX/wWsrIx//hFgDke9tfZJ1/HI0HPc+Pi3zjYx2ssNY/2htAZT9Q+efyKvyj5/fvbjWoj+1eC3rkn5W6XbYx0s4L2qJ11zSBt0F9nZ6nIaDKzvb8Cql/EBppR+1DcAAqfMYAFvOOkiS36DgcUJWADZx3uBEjgmYAGwrNxlFyzTQC0CFgBAZX8DlowPwJLSD1gObVRkBgt4w0kXWfIbTFcpm6cNU9d/Aatqxnf7J8yiZ289/lsjtyT3b9c+GHfb98UG54zdOa6yDY5rcXp63d+6JuVvlW7PjxksGR/44sIKWXIazBj32+P3Rr79IZR7fdnz/fY4ePxYK4StHA8IX5/pszGt3ZMXa1hml6tLazCDqT76+DikvHSSnBY5vPsAuS6+kbDByNDh4Ff0NuIGh+f+Y+awNxK22dXBbWYq8csq2hZe357XGaxvf1/GB263f6NBwUnXTI7nsVbd67rut8ftdn+8/hD2kj4pnv6FwIyaRodOp4wJLr6QuFm0GjVmhijkehbbnabil1W0Lby4PXUD1mKtEHYTJ0PE2ZKnaNtTxWI7tdjuNBW/rKJt4ZXtsQ4W8KXFsSTOA9sl3xlte8JuwMCdWmx3mopfVtG28MrvVgxYqzZE2EqcUTLOltT6hnbfdtliO7XY7jQVv6yibWH2N9QKWGs3RNhKre4cJ9Os+j1VLLZTi+1OU/HLKtoW5n3P89MlFzt3aIWwocJ7IOLcjVF9jIq2PVUstlOL7U5T8csq2hambs//yv78Vq0QtrLG1b1oVy7CjpmL7dRiu9NU/LKKtoWp3/nyOauJAi+yTh+bjgxxtuQp2vZUsdhOLbY7TcUvq2hbeLI9n7bg069t3v5gZ4cLcHYVZ0ueom1PFYvt1GK701T8soq2hdYfBgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgBUEeXEjAEA/L69orp6HBCwAYCOPz/9UMRUJWAAt9BnDgTwHPfOpVv/UzwEqOh29vzMCQ1eJ/bNKz9S9AarIilbfGYehh85nP/8r/gYALqerwt8FghKwAAqVJyQZC5p65Paxx9f/uUrAAihRKxvJWNDImAnmPwV/FWBzdVPRw/1YTKT1OlKVVJlgvrJzZrAArmkx52Qeiwk83rXUtz8cbeQEs4AFcEG7Q0m8gxR8c9xAIzXf6hPMeQQsgFytDyKRDlLwTUrTjNF8X7finrld7z6f9w0CFgBwLj1fjM5YISaYBSyALH2OHaOPUDCrj30nfRLr8JOpXyJgAQAnLq0jFU5Kxsq9mPiJgAUArKE0P6Wlq6QEZh0sgHTnA+vj55I5BWfDlsWCJp69sl5XfU/AAqjj8S4PPX9YfewGCrXulS4RAlTwNl0l/itQQ8/TmPO/JWABlErJTzIWU8ttvpq7gAVQJD05yViwDwELADiXfn7gTOImYAGUyJ2UMonF1FKaryb+JGABAKmO85N09cUyDQBAhmeKerz7IV8ELIB0946PgjtgEZoGeswlQgCAysYG0IMTQckYPtFxxnot/6z71nMWj1abkCvQBPOQS4RZb1I0xMCTjhNEn6uEahDm1vkS4SN/YLrwK7AYHSe09EkpbySEffQMWCUji1GJbek4Ab1OL6UkJxcHob0+fSfpr3Trxqkjy8vdDD+HJIMOu6mSkHScRjJuxpKuoJcoV/D79OSkvU0bmww97KPiMKHjNPKmjg7PEk+pKSjXNGOldtIOnbk0XT3JWGym+gCh4zQiB0NAjTJWRidtfQ9WnXT18zNuK2F5LRq5jtNIrVQkXUFFLTpU3nc2DVjV0tWvTzpUsLB2zVvHaaR8KJeuoLq63Sr729oFrNZDuUMFS9JxJlUylEtX0MjICeZ2Hbvy9NUXN2OxtCsdx23UkagLiKbwxPJiP23UvS8uypDCwg2s67zjWAhgHt5oBHFczljXe+uQV+UAVxyfkDxudwuFRyJFQRzP/th1glnAgjkkPmwrYwF8kPKoXLVToxbnWBnje/ElwpvTRFZx1HGyekpaxtJxABrq/LLnV7ln287OAYD4Bgcs4FTuRO+FiWEA6hofsNInpUxfAQBTGB+wbmnJSboCAGYRImDdzvKTdAUATCTQMg3PFFW2RDUAwHiDl2mowf28rMEyDQDriHKJEABgGS0CVs8zY2fhLOOoMdd+2FbHAWjLDBbMwcO2ABMRsGAaHrYFmEW7KwUdxnqXOVhPUscpe9hWx9lKj5faAr8FWqYBSGSyijMpLeTrM5IW1Ne0XzU9BhgRWJWOQ4nL7UfbgJqa3oPVrrsaCFiYjsNlJenctCjU1Pom9xYDuoMEy9NxuKA8IclYUE2HpwjrDusOEmxCxyFLrWwkY0EdfZZpqDW4O0iwFR2HRHVTkYwFFXRbB6t8iHeQYEM6Dqda5CEZC0r1XGi0ZKB3kGBbOg4H2iUhGQuKDBl/LYoIF+g4vEhtEgUr02pIcNHYzmOJYbhAx+HpPCc9PjcJ7wWHpnQegBkVpasnGQva8bJngAWdpqvEzwDX6F0BuQD0pBzgwNHkU1ZyOpvH0t3gCi97jsPLWZ+UA5zq+YjfQ0eDC3SbCLyc9Uk5QKJq01dPJrGgOjNYwxW+nHWZgU85AORyK0VcAtZYVV7OukAvUg4A6dxKMQFPEQ7k5axPygEg0SN/rLvwK1RgBmuU6i9nnfQc5bUcCpacvs1cDgCn3EoxE8U9RKOTielq8/HzfxQuOf3t47C4kx5RdZmGmz4VQ5UDh6rsxyXC/ryc9Sk1XZ3+6/E3A8zPrRTzEbA6a924Z+k8Gekq/TOfvh92kz7pmzk9zBDVbymhBwGLwdKTk9d6wD/nfSElOXkX4Qxa5CEZqwcBq6c+bTp+z1EO0MNxfjJ3NQO3lEzMU4ThlD1GN5ncSanH7b52gUBdz/6y1aiykA63lJiebEjACuRt2nj+0IAI/HRPPwAXDCAOwHCRgNVN0WPVmTM3kc9LvKQW4FTDofLXjCZNuAcrhAaP0QHLaz0mGHNW87jdfx9KLPTeiBms8bIeo3OtEPgm40Jh/jezlLPrJEGqfJ3XV5vBYqTcvChfwi8tjjqTHcmWU3+gS7tOMtAjYSot5TOBCFiDXXiMrtGWANOqOywYZFaTc52kv2VfXy1gMZglp6GGWqlIuqKnwtdX/zepFTBzuQeL8e63x+kJlnRFgnXu3rik/H6sHUppO/nXSbq1g6Lm+mFho9stTDs2g0UIlpymwIJ3b1xVcmQJclRiE/XTVZ2vrscMFlFYcpp8F5pHqFPcFp67llUyC5cGMTVMV19/YHizHr4BW3nfpLLmb5d4OWu3zBS8HCjkcliKzS+bTqrmIHnh0ajGLaN5uvoytombwQJmVH4EinCK28EO+8hENroo4R6s8TxGB5lqdQQdivXlLzfYTmmPu3DD/kACVk8fW0ZKB0juJPFPWPtsYfxy4Jq6Y6aMRUCGrxUIWFF4jA4StOgIOheLy7lO0s52HU3A6uyo9d5vj9/d4O0Pr31/JF5SywXtBujthn52k3adhJqU5xBezvqkHEjXIQNpOYTS9Y2E7Vt/hd2J90Rk0D+9ueo9Z9KqVA4kSm0qBUupaTyE0uqk4lcf6aDavky0TIMBZaCKnWfqelQOnEpqJIdn58s8I8JWmk7c9mztOwYs92AN5OWsT8qBCs5enaF5MKN27XbWHhHjhv0kAtZY5Q1geBOqQjlQJO3VGRoJM2rRbufuC7PcsC9gDeflrE/KgU9OBtP05JTwSY8TElDdIW6FAfNsYaMQvConAi9nfVIOAG/dK6X/dcbMZ8b68FBLiN0MsRH85OWsT8qBp6PjyqXHto8PVFoXYc37gvPOc8MherEZrIBCtIwAlAPAdyXzWEbU3gQsAJiFWymmIWABwFy+MtMst1LUuocs8W+FIGABwKSihAl+s0wDMLGcN+Fc+TzANQIWANBan8m2QFN6AhYQ3MmImfPqjNNPBhqdgakJWMD00l6d4eIgjNX6BCbWCZKABazg7NUZ0hVEsNHrq8NtEMA7qQnpw6szUhgPoY/qJzwRO2/EbQL4pcMUlPEQuqnYo4P2XJcIgSnsdfcGrK5Wj4vbcwUsYBYb3b0BGyjvd6F7roAFTKTFeBp6jIallfS+6D03+vYB/LL+3RuwmQVfXz3HVgL8VCVjGQAhmlleX31uss0F+KcwYxn9gIbcgwVMauW7N4DZGWWA2S149wYwO2MNsIx17t4AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAf+6jNwBYwOPzPxlkgB0Z+4DLDnLVb0YbYCOGPOCCrGj1nTGnOtOHEJHuB+S6nK6eDDtVmD6E0PQ6IEthunoy8pQwfQgTSOxvpqCBW6V09WTouMb0IczhuLOZgga+VExXTwaNXKYPYRqfepopaOC76unqyYiRzvQhzOR/735Y0o0bjcLAQO36tREjUd2CUuzQ3O+AVd7xdF1YSesebcQ41aKIFDu09TJRbAoaeNHhSGy4OPD4918/SunufiyI7Xvvcgcr8KLbPIfh4q3H7Ve0+q5GzFLy0MTXJUJT0ADhHKSr038FBnoGLHewAkTzSMlPxRnLKA1N/M8drMAHPTuvgeJVenIyjwUBvV2mAQCA6+6JJ47FD7A4wYLpdJ5VMkr850LRl93wrvDhk4tvC/yT8MVvfv/5w0rPCQMAhJKScL4+8yYpnVwi9AALALCTR/4k8ptfOQpYVR9gMdcFAARX7W2BHwOWB1gAgJ3UfFugpwgBwsm9w9UdsVCsVif6+z3vA1bupJRJLABgWnVPUR43M1jAZz1PnJykvUqflDJ9BWWavC1QwAIIKiU5SVdQplUPErAAArr/+6+j0b9SujJ9yLYanp+cLzQKbKzPyx4c4I88C7P4dRpAVx9Hz6z71tO6ujEUZnTSuw/GiuQQYHB4q1uEUv7sqW0X6zaDpQPDpI4msU5f9pCQsQwOn6ROHxb/FdZz8fV5VHR96Pz2FaavYHlvunmNIcLIcEzAIsuel+YvjzBt+9fJGdLpAOoSAGzj8fN/lN9FYFhI0fQYoAqWcbmdzNsGCtPk+a8X3vh48hRhpQdY5q0/4EvdjmxYSNSuoFTBMqq9Pm8SdV7GfPjp++9zyLc/POARISDLmyfaUvwcNIwJuaofBVXBMnZbqqNwf7/29Pr9UYkR6H78Z2qYqNqAFI/igPXu3zlRcaBW2svYrVVUTJNFSyikZKz/SVdAphb9+nFp2n8rtYrdsLyM+q/Pi63yy5jf/UNq70j5ZNOV3HVjIJeYdaB8UDUsL6PJ6/MafGct86XJduus6MawrAujRv7K48aQTzZ8XowXSZe3ri73H7Cd9MhD1e98aBSwAlYPUFPmcy7LDPRxeNJoW3mL0l3qfaEaTKd5teoBq8UlwlAVA8wr8gWL4e7//lP4GaZ3+kKFblvSwMSDQPUZrKkrEsiQvA5erceqgdfeVPWdKz9/I4R+ASvyDJbzJNhLSocvTle3qU9hoam6T73FM3ffLw9YpqBhX2cXqJo/Vg1QRe54dfr58oBl4IOtvT3But8e9dLVk6EGrr8PNP/zelypP6M3AFhB7TgFbG7AkHJPfk1F4kru5QyssLk+g4ChBmgrJTklnk82XckdAGAmZ8uHpp7puURY6KCg3fgfkPoC4MQzRZWtjH+vuA7WVscnayjPRX011fnKnQpiWxcXwXqabSms/gNLxb94v5nBynehAp6/4qgwhPoCIEWtjPX38OEerCwlRe/+3P7UF7Cy9Ekpz/mmKT+1/u8bBKx0FqSei/oC1lfxqTdut1tZxvrxuwJWIgtSz0V9Aet5f+yv9dRbyt/aw4X307z5FTe5p6h+lF24rCJQX525yR26OepuZU+9vYjQ0bqNLcc7e/Hxcze5n2pRwY8YbXdJ6gvYlEuBbVwc/10iPNauseoGLagvYG19TvacUlYgYB1ofUx1zK5LfQGsZO40WStgSbuwrZ7d31ADrXuBXlaHGaxPvLx2LuoL2Ee7DBQtXU2cJgUsAJhOi2QQLV09zZomqwSsmFUCdDP3rRIwp7o9InL/mjJNmsF6q+eVIFedyqkvYE+1UkLkdPU0X5osD1jxawXoYOJbJWBmNV+fF9tkabIwYM1SK0AHs94qAZOr9vq88GZKky4RAhVNeasEzK/O6/NmME2aLHkX4YwVc+6lOLq8eWDNkuyo631Rv972xW8Va0QBwwUXX583laxxZsBeXw5YoWqoTks6/Jamh/BQhTmjTgHr8bmm0qpwhyHvS5VKWa9YgOqCDq0XAlacIa9mej39rpYZK06RTqpHwDpIV0+f/zn6aVYzhfWyUlEA28kNWEGGvMqXNRO/rlnGClKq82oesE7T1dOvD7n+rgSATaUHrDjjXeXT4syA2eJYHqdsJ9U2YCWmq6dvHzV/82XGs7gsQa9QAAN9df5PA0S00aH+jR2jA1a0Ep5Uw4x1KWC5A+mtxYLI8sERuG6uPl//0aQL31g7Y81VBWG1ClhZ6eqpagvRPGJy6RM4MdE6WHWPoF54whQ01IBKKkWFwi5mCVgtRqUII53T2VoWLskIDZUv5dWhQmELUwSsduORkY4paKhB1KoIFQrrix+wFh6JFp50GWLt8ly4I8zCXQpAhvgBq63c+5Hr3b+8dhoYpX6pjmshhLLqXQpAK8ED1qoDkHTVzsJlu2p3iM9dCkC24AGrh/Qph0qTEwsngCDqlvA9/etMX62odZ1qM7AmAet2SzsgS1dTqVXO9/Svk64A+BI5YPU8XD2Oj6D3r/9TRLrqqXJ9nbWQPs1VhutMtQIX/Rm9AYE8j6CPdz/89r+s4DyRyvX1oYU4OgLwSsB6dZaD3h5ky76ShurXl+oE4JSAdc3pS7JvDsSRqC8u6HqXghYIixGwChkT56K+AOgh8k3uAABTMoMF8Opxu32f7/QoA5BLwAL4z9sk9bjdb2IWkMMlQoC/jgPUwz18QLLIAavnWGbc5DINdREp01MyFpAocsAC6CT94p+MBaQQsAAAKgsesPqcKTofpZCGOrfce9cbTGKpXFhN8IAFADCf+AGr9YmdE0eq0FAB+E/8gHVreWhx0KIiDZVr1C8saIqAdWszABnUqE5DBeB2m23srriM8lw7zlw01G4Oijqv6LLqrOqS7qoY1jRd364yrk2310xHQ20qMw7V/8ZaAUsVw7Jm7N6FQ9uMu8yMNNQWLpfqeXkmfrV0BaSY5R6s70pGJSMa3Wio1ZUkm/PfTSl06QpINHUnr3+ZABrQUKvodNX17K4uF3+BJGv082o3ukJLGuplvW8qf/l796N/bLIBwOx0dSC4iunqqXzca3grGLAGvR2IrHq6eqoy9Ln4C3ykzwNhNUpXTxVHPxd/gVc6PxBT03T1ZAAEWplxmQYAgNAELCCgDtNX3f4KsCMBCwCgMgELAKAyAQuIpueVO1cJgSYELACAygQsAIDKBCwAgMoELACAygQsAIDK/ozeAABgrJP3aT5+/4gzAhYA7Ol8mZK3n3j+UMw65hIhAOzmkZaujkKUReSOCVhAND1PjJ2Es6GkaHScrjK+aFcCFgDso1q6yvi6LbkHCwB6OrmjfNCfpjLT40BMHY4EBkB6ymrSLRpn6gakT1990Zd+M4MFAE1dOFuo/qCeuave3IMFxNT6lNgpN32UJJtaqUi6GkDAAsJql4GkK/ooTzYRvoErBCwgshZJSLqiD/NPWxOwgODq5iHpij7qpqLL33blF++Zv5X7+U24yR2I717pcCVd/TZwyYCFtQgcDzUyF7UFzKLwoGW4+274kgELazqdk1sX1zcmcbGGe/0HHhehRICJXD5aGOu+KMOmQq3fVroxpxnr58VBLeQH92ABE7nnD+IXfmVhEZYMYBrHN1e59eqYcQeYl/uHsriPrbVugSOxFqptz8tU1odopW38oDgAdlDx2O/A8cmyASuNhvGDS4QAywuyZABsRMACWFujJQN40bNMlP8ErIMFsLCmd+G4JAQfCVgAq6qTrt4+q//8oYQFn7hECMBHxyshuVIFnwhYAEuqEH5S1vKWsQLrOcNoNvOVgAXAG4lvSrnJWPCOgAUAUJmABbCe5i+hq/z3aKXPlTvXB98QsAAAKhOwAKBc2DvKW2+Y6av3BCwAWFu7DCRdfSRgAcDyWiQh6eqIgAXAq3vmbeuOtLfbLfwd5XU3T52fELAAYBO1UpF0dU7AAuCN9EksB9tv4t9RHuEbtiBgAfBeSsbKvZi4gfh3lJd8j3SVSkkBLKla7jlYdPR+eziOfFA9d7Yo56yNVNF5lBfAkiof4F9i1reJK8eRTypWQetCPthU9XuRggNYUrcrd44jB6rUghKeknuwAJYUfMmATbijfF8CFgC0447yTak8gIU1vVDoCJLFHeV7UYUAa2uUsRw+LnNH+RbUJcDyplgyAJbiHiyA5XkJHfQmYAHsIMIi4LARAQtgE5YMgH4ELIB9WDIAOtFhADZkyQBoS7cB2JklAwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADiuI/egGU8Pv+TQo5ABQHQj0NLoYPD9m9Kuz8VBMAAjiiXZR25v1PmfaggAIb53+gNmNTlg3fh75JIBQEwkoB1QfkB2CG8KRUEwGACVq5ah16H8EZUEADjCVhZ6h50HcKrU0EAhOB+3nSNDreqoBYVNIGXSlK4wKqMb4maTmaohXIqKDoLkQFbcYkwRetLRS5FFVJB0R2XoPIF1iNgAW2l5CcZC1iMuflT3UZ+dXFNRgU9fhby3Trv7SliYE9/Rm/AIsqO3DT3eHfsfv5QZQFQnUuEpR63+++D99sfMspxXaipdnKjq6gLLEPAOnYy4Fc9cju4XHBeaCm1kFZTKgiAVC4RXpd45HYFaqD0jKumgNlY/CQ0AesiR24ARsh6MFfSGsYlQpaVe3OVm7GA2B6X7mx0hj+GgHWFIzekyG33+gl8VpKTZKwBBCwACK48IclYvQlYQEPpk1Kmr+CDWtlIxupKwALaSklO0hV8UDcVyVj9eIqQZd1vj6y73zzs2c7dA+VwRYtB6bF0nws00ixcyrW8r60GR251cc1R2VatJhVUwUsRK1P4rOkp32KdL+JbT81gsbL0SSzTV30sNqhDM61HpGXmsS4U1PNXmu++e7AuSj8eO3KPlVL+6ghgQqGXrhCwTn0MuVWP3GucSQxxXnTHteACLhBMn1O+2U8soy9dIWAVqXHkpof77fG7Ot7+EIDwJli6wnl5opM6eLnRJ/OwrRbKuRsUWEa3E79JB7fq5dOkHCYt3CEatXhVUIsKAhbQeVp9uiFumqHeJcJ0LVrhdC07MhUEsLZ26bP+NwtYWeoebh28q1NBAKvqsHRFTQJWrloHXQfvRlQQAOMJWBeUH3odvJtSQQCLmW/pCiu5X3P8arXT36U1FQSwvrJH+NtyLClkOYbgVBAwF08RvvVaLAevQSuLWdUKZJaSjS/QG7x5RwUBUxCwfstIV08RMtYUJQsAmxCwfnv8/B9J21yQseqUiZvcASCOnolninT1Q2K6yvpkIwIWAEBlAhYAMIHcSamxk1gCFgCE0icWzHd9cC4CFgBAZQIWAETTenrJ9FVzAhYABNQuA0lXPQhYABBTiyQ0Y7q6//uvvKWtLi2FVa18BCwACKtuHpoxXc1KwAKAyGqlohXSVfqk1PAXP69Q3ACwuse///px4E6OEQsc7h/f/r9G7yKsWUoLlDgArO/wlfXHeWKNY33qGwmHv4Ww/ncBAC2cRoYPqWKxo/zrPl6dz3urclktVvQdHJ5CMB8VCkSXfBXw8d//u6xGd1bVL7GF66CurBpVqvGpUGAORqtfqmesJsW2R10UCXEpl3pUKDATAeudihmrVZlZpuFYSRUOfkCUd1QoMJPccWebcWqCpSsErAPlDXWbpj4HFQqwjPJs1Ha+T8D6pNah1CE5CBUKsJiShNT8aqqA9Vbdg6hD8nAqFGBJ9/yodOFXrtjmfrgM0zwCShoVCsxq5MuKpxRo5Z2tK+KlHu5vflbX1qU9SLUKfbecnQoFmvMU4aQ2rYuCFw4U2rTAB6lTlYcvZABoS8Ca1I73YB031tNXSLKV4/bgbiygtfRjkqNXKNsFrJQjYsuM5YjcTYWiTmkJahRoLeWYJF1Fs1fASj8WmscivQ3IWEBrx+ORI1ZAf0ZvAABw7pmi3j2eRUQbVc2lh1094T+p0oq7MIWpUgH4stclQgCADgQsAIDK3IMFAFQRaCH14QQsAKBE2gpIf+2StDa6RJj/NkhP3+8rt/Z3GTAAfnjkP1R04VemtFHAAgDqKclJ62esvQJWzgsH1q97jqW3AdNXwH7Kj5KLH2f3Cli31BcOLF7rJEppCVoLsJ9a497K4+d2Aet2/sKB1vVtvqODaoV83B7ut4cKBTZT9yi5bMba+tjw64UDfap56zLvqHJtvqztft/viRhghGgLH3jBSaoFd+mqbiFamfehQoF5ZY1g3UahpuPqamPpavtTpsMhWYH3pEKB6VweuFoPR0bUPDvegwUAIVn4YB0C1nets/NS2XwGKhSYSOSFD/qkt6UyooD1ot0h08F4CBUKTMHCB6sRsH5rceB0MB5IhQLBWfhgQQLWW3UPnw7Gw6lQIKwWeWjexLZOOhSwPql1EHUwDkKFAgHNftcUHwlYB8oPpQ7GoahQIJTWGUjGGknAOlZyQHUwDkiFAtCDY0aimIvqcpkKBcaa6G0TnWfCFhly/4zegFl81Xe010I1svxu7lahAHQlYOVa+6Cbcpqy2EuO19gLAGJxDxZPj/xJ4Au/AsDNwgc7ELC4efsVANTl+ghVEpKGBFyz532Q0902XmGDHz834/7+O9ep9HX2hEsqdnJtCUi3+ZO8ewWsx+cN+BWzOtd1w3y/XqslXfUerjkBpy6PPCuNMBsFrIN09fQzY/Wp5R753j1Y24r/9itgPe74nNTFkHGarn5+pkO66vdE10onBKRrOk5pVMBb7vj8Mt0M1u3CNqekqy/326N95ZbuQtb2mcHakLdfAf3VGhmMMKO0Tj+x0tXjdv8dELPmsgQsAFqrm4pkrFEyMlDW9NWteaVmp6vy71pjrpV0E739ilXt+Vj+zhoNO7O3lnlH46Qtzw1Yt4Y1WjNdfTn9kBksoI9Hwvx6ymeYS7va1E5GmSvaNklXKd/rXYScSFsaDg5caDPPX5lrHOe3Dnd8aiRD3CcJuCM30gzWVqrc4vfmh+V/i3V5LB/e6hMN2/2V+NG27WOPp39AwOK9s1v84nctIihPSDLWvPrUnRYy0NGBIPdyx3oHFQGLNzKXhoO3PJYPx2Zf+OD5J2IeC8aPG6HuwfJsUQg5t/jd3ZLV3qT9ovpj+ZF3Fi5rdzNTzy7z9bceP3/6qPVE3owiBKyUtvX1mSVrAX6bul80ehFTtN3kQM9Tr9nbRouMNapAXv9uyr5NXXkHxu6XV352dl7gl1Yu+fS1quma2fuFFzFxm/NVMGNVLLEhpXE01959Hr7hq6m/+3f4e/8rA+/B8mwR/DZ7v/AiJrimVtLonK6S1re73x6/T8XD3r1Vy6iA5dki+E2/gJ2V542eiSV7TeD736QVOlql31h8+skhAcuzRfDbAv3CY/lQqCR4dE5XQ363h5SMlfKZ/gHLKz8HGvJQCSn0C+DpwvxO5ymh9efaj/NT4ixX54DV6NkiqslfGk75l9MvgBcpV9KGXG1bYK49yYf7xt788JOeyzQ0feWn+RImtUy/8Fg+tBCqqW+3vl3JJEK3GSzPFgVx3ppr3OIXvc+EoV8AszDXnifCQqOEk7L8rouDo7xUjYq4ZNL18SfSboHyt3+L1paZa++nz17pZtEk1chBxjo8qKuFRDXf9B6gRqZYWDJrI7XkQkb+ZXSoyuqVOL75ednznhJfDnXhFj8jXSvHc4revX0me82eS78C8FfEU9uCKyCOMVmWefvVpDLKP/mFqW+/M2I3L5a1U4XbpmFfdlLyNa53q53Wxk8FXTJ+szs0zTpHkeSOp7Nlmf3tV/Oqn66exmWssAGryoZp3td8LPwao/2/j9PW+KRy1eArm4EuEboCMsKkb7+CRLus2RPV+5Gh3mhv5CGuKAErpUfJWG3M9far7eQ2e93kG+vjR/DaIOuN9pp6B53Xt6urdQs5+f4QASv9kODg0cYsb78ismgvYrJmTxz/1Ve90d7IQ4p27eT8m0MELAKI//YrSNd0zR4uqDtWGHlI16K1JH3n+IVGL1wBsbJiM191YSEGwvn1xNnnD7beEL3ginv+OhlvR3uFT666y96mtsDxAYuQDGFc0Grx7renYY9/f5KdqHCuqTU6ZbRAlwghtNz52vXmd8+eODv7QROrFfIkpCtK9H6iS8ACKqp8CEx74ozlueOTKro+0dUhYEV7tggiKF87NP2TnftFtT+X88QZqxKtqKvfE13jZ7BcAYFTKc0+UtcYdUSces2e9V04rEEb94TYlPKZI25yhzncb496bxfpoPSW0vznix2PobpWT658+Fv9NfyjfWawTnag+ApI0l+BYLJb7P32+N0F3v6w5K/Uo0vyyktw2EeUGazjs/Ovz/TZGIhsqo7Q8/SXOaS0CemKBXS7B+u8vxwfNs4OKvojMxr8qqwuImxDJ4+f/+ETL8GZSp8KWbDaO+9S0pjza71mZzusrdGxOFq/yNrN7FW/bwPm9r6/Yi/hQ/zyUm7KKqoOPWvByu+/S9O9Lhs62KpfpKaRvESW/Rvl/m6tU0A20LRzrdlFhuzVgFcCQXj6xaspAlbin1ykStjaJnPt1QxZB6tWaS5bK2xJv3gV/4mz9AOOW7KYX4t+ts549duohUZ7vxIIZqBfvErZn/vb/7e51YoaEtRt9ot3ooEruXd9JRBMQr94FfaJs9xJKZNYLMFce6oIe5h/owWsT794lfbEWb9Vpy8955jL44nEVNjRLrfemXpEqA2aqeCgF/0iV6dHylsGLPGa+C53tAstdsoeEWU7ACqZOmD1PGhBudbRZ+IeMX4LAGrrsWZPg4A16rILlGsx1z53j9AhgSX1WLOn6sl7lQ02pLOM6XvEwKcIAdqZa82eWnHQo4qsYYUeIWABq2q+Zk+lpVDrHgNkLGa3SI8QsICFNV+zJ3Mp1N9ajP4yFvNap0cIWMDamq+PX7AUartxX8ZiRkv1CHdEAjvo8bB32lKonz7eghGeiazWI/70/GMAgzwH1rZr9ogzwBcDArCh4evj93ulT68/BCUW7BFmsIANiR1AWwIWALCPThPYTuMAOuv8QJNxnuACPvRaoddYpgEA4LtHeewTsAAAfivKWAIWAMBb1zOWgAUA8MnFjCVgAQAcuJKxLNMAALMbvnbu8h65JSlgAcCkUmZWvj4jaRXKy1guEQLAdC6sI1Bh6YHtZRSggAXQWc+JBJMWSyrJSQEz1pqtVMACgImUJ6SAGWsiqaUnYAHALGplIxmrOQELoL8+10TWvPKysbqpKFTGWrCtClgAEF+LPBQqY00kqdwWzIwAk2h6eDO8r2STpjJR4HsW2tHyY3GKFWBDjY4oxvaVdIgdcRrMRBnrRJwyBdhT9SOKgX0x5y3k8bPS79mNKlSbWSRjhSpTgD1VPKIY1Rdz0jYen2s8M2aFajkrZCw3uQMMV+vYFuoYSXMH6er0X2Obd8v/I2ABRFB+RFnhmES6lPwkYw0kYAEEUXJEmf5oxDsfr5SlJ6fkTwa8Kjd3q/4zegMA+HL67PenX4ElXegRUeiZAGEdrbLTbysY5n0DuHDhL+2G9/iN6rhHxMphZrAAwop/wIOeZuoR7sECAKhMwAIAFhBrfkvAAgCoTMACgJnkvgkn/8058wo0iSVgAQDLiJKxBCwAiKnCewaTPxkll9QQYl8ELACYT0py2uni4IvxGUvAAoApHeenjdPV0+CMNT7iAQCfneekl7Xd86PVwmFgWMpcuEwBYAEdIsKCYeCl1PrP5y1YpgCwlqbhYLUkcPjCwn4xyz1YABBcuwy0Ubq6XXpP9mUCFgDE1yIZ7JWu/n2m014LWAAwhbrJYMd09e+TPfZdwAKAWdRKBqulq4AELACYSHk2WjBd5d673mESS8ACgLmUhIMF01VMf0ZvAACQ65mTsiZumkarw7URttQ2YP1a5gsAqOXruDoq36Q9t/fXXimg1d6KssA8jFiQ6/KKnU361IWtab3o6Jj9NGKRwgwojcW5vAJzKYwm4zNWhyXd6+9k4iYbqzhgPoHGYp18b0+Pn0uVaDIsfvz787MFLOeDlDMDSmMRT7635Igxo4q5ZOAUT483EgpYxGIGlMaCnnxvps8Moomx6qrnkgEZq9v7nmvu26VbzOA/AjqNhT753kbrGUQDSSONcknXjNUtXd0ELEIxLtLSBCffG2g6g+jWunaa5pL08k+clXzcfq3V3jNa/fuL9QhYlNB+aGmak++lNZ1BdGtdOx2iyXH55559985S79wLFxr1HD1MZ8MbU9qNto91C626i7XwYR7i8XvGooyqjOlCzfZPV+9bzrX29H7rL7w6UXPmixmsxra9ADv85JvbtVo4OKb8i1kXXhdzTFX+1i2vVJ+V7ONjm8l92fPjYIdzL3BqyNDFUbet9yvwSeV09e1fqzdUzT6O6esiK2BNv7ewn5Juu0CX77MLCxRUO/XTVfpnLlGbEaxQC1Vu3X/5XNJ3mr6KbcxtOttexGpm+JpPw+/3Gnh1g6fsKshKTs0eDatVocO7QKHOQaf6Nd/Wjiox8Sb3jL293x6n3WOKZrUl70VfSa1B6sLNvxoSTxMdKV8U3vOuC1w2b5t5lVKv1W9OJKAo68dYyb2SUWs+RWlIt9tt0Mk337WdvnoKNokVqguUWyfuNHBSZW/vwXr8/M+1P/v43ej//VCFRRPoNp2UMSbmOBRJiy6WdUbe6Puhp84LBOgCczk/ENVdRySLo2QQw2/TeWP22xaGGrXgcsCGZEwbq9Xt7S9artBd69WHjf5iB2LfW0nV9JzBGjKrpNoiqHibTk33d+337Q/5qXW3+vT9QRsS9KIL7CP1QPRHdW6s+voxlfOPOBXHr9W0f/5j3T+l5okisTXqAvvIesR1cMDSjEYJdVsoVTSp07MHViI3JJcIb0Ovt9d5N86x9m/wPd2YyF2gnCmY744j02uV5a7kzhqavpqNdZytpt3yL1Mk5SmloieZuN1uxtLN5HUoAWtDo27TYTJDV9O+aUhXTf9mpPRJqfbTV8d2GEuDTKTV1WHu9nELELAiNCBYQOWulJ6cWmYsci2yTEBKchqdrpjaPeG5qcKR7ZG4kjvL6DMkuUmTKgobUs97TCM0+PKdDdRzj18KEiBdGUvX0HDpGQHrAos0rUE9fpQ7KfW43QMc8DY38M1IrTwb1a8nWLW0zoY/DDdEhV0WsNJ5t9Qa1CPriblMQJ0Ds0RFd3Wa3PB7sKYw/U2j//TcpJi7v0Y9bqWw/PtE5LFBfNSbkXa221i61almtQIXsE4tctPo9tQjS7JMQFNbBYtj7YoiVCHXbPYC1rEqN40ynHrcWevhe+DhIf4yAaGOnRRqUZuhWkjlDiVgHfBuqTWox2y5d72Ev0tmk5NvaK1ug7//+n866Pe3BKxPqt80yhDqkaf1Tr67LRNQaN4MOu+WN1WrWKIVb/0ONTxgRSviJzeNrmGreqz+pu1ZVtNOd72IHrf79/8Uftt+lNViyit0iyZhmYbfmt40Or5VbbOoTJ39eldcIeqxg+OVHr8+02djitV88fDzh1s0gmqmW0tJ9R4rqdC3ZdunhXSt1uGNPlojXq2CvztcWLPFjs998+/ZKtLRmu6tUesNsJp2SVEXbWFCvhyl87hda0crbva99hf+/vIDk5Z/C1lFcbwjA4+/Tf702IAVsNG8nrI3OISM2evT3VhmT2+3W3mrDnxkPdaqOw+d+LxW2M3bwNOgljDvAb7Kln/fniED17zl306Vd2M0LdiGb8X59PcErL86TvD03vHEra+6myunq6dYzfevbqeAwd/x16kNPI1oCVMf4KsvHjtk4AreBeY1ZFayyR8deJN7rEZzXLq5r2YLJb3hTL2btaQXQsg7ShZe8yldyJrhP4Uv8K77hS2+hxLrPPA7KmDFasdpb6eLtc3wweZrPrW9/a7Vn9zOPb85Hf+K59pW0mi1rd6GBKxY7XjEBE+/ATn/3Xu1XvI6Stubmmv+sYY6nALGfMdf1ArhvXtC0kr5zNcnS7akw69cEOtw2dEKs5I/lmn4fXRZ68Zn2Efd2yun6LbS1bzqHk0rPtfGQOWD2ODK/S9gfVruZZUbn9+7MMEzz8I/o4Sr5V3VylifKrT1IzJZDUmv5MtXy6nyXNvxH4rTBQq1Lqtrqq+21dXfgHVwZaRepBi/t7SnlkOpcgD4+obfldvuACNdndhmxeASHYajIF3gsrQ7kP8aMrxPPCv5v1vCfSc17suJssO0pJYDqruC0e9hbp1Hfr79+dx3XXf1eDcmf3uHT3X69bFJu8Db7lz9V2qpe7ve29+t755eWgVnSHH754Vdmm4+LzP5z17Lyy+AdFnTa/0DbyTovX79b0Eu5Dy5cXaQue6lmfvepjZmfdmzuevlLdnfFlNyevei3TxWlIaU867rfiwoE9hEXaDibQNxPD7/Z5iMgFXWbwPWx+12ZSWWoDtyIH0fr+7dUgN6zpF1vsZQrye2yFixGlJK/UZLV/8+WXG7YlVKbFN0gXYjwBApKSoxadUv/J7rYAWpjx2lrSEjXf2VdmSdsT3X3ea6Geva77atheNaXrDpU6p/F8jSegToaYJ7yDLuwbrVOahEHJS6v6pvTCEkPIY75ZMav9SppoNpgH8tIWwJvNXzWaduDanTcPnrkb3eBt0tOlcLDyXgWHq+SZceTa31wG/PB4cP/lbNIaV/wLrF7LR9bx0dWQIvu/FhU2KuiZKoZg85HHHiF8WXUav1tG5Inc+hh9X4iIA1UfOOLMhYetIYEs4nD5zuSN242TqHVBtVhgSsW8yue9gP6o7jEXd/Id0OurPUY4cCGVUUAtZHZaPWLG2bFNfT1VNBxqr+WHqfmY46fyXjHqw5bzrJcH+3j/fbQ7qaTZ8SVo+sStveSMojEVcfmyg5dL793T73kFX7K3/OP9LEI2wfXj5Hspk+7TlujyaTelzM0QiQnpzO3unyewSoshjE/ef/rK7FZv/nf4mlu03saD24GLz6UI8spdeCMho2VVRfDKJdAnl8+P8r+HO73U7fpbRNuno6LY+Sb6Yb9QhZNOy95F74S34xccMLeW1ewfloc7y4/70H66CYm6WryKFt0ndL8UI99uxlkXv0CnJWDL7w3XM1bBL1HwFa/cW3b9us9wrOJtcf/7vJ/e02bjZ39V3d4cbgNYp6XFvPGhlf+2krBn/992lsqvj2JLg1TVeX/3WgHze5bxyn3qo1Zxi07rehHlnHcWt+10a1W+aW+JxjwADT81U5M5ri3VKcUo+s4+2kk5kolpT1nGPTLblAwDoV/N1SJFKPq9p02bP7z/9Artwpn4BTRMEJWCkujGAGvYDUI8A0Ljzn2GhLrhm10OiMvmouyLuluEY9rqfdkhxf3w8Lut8eiaHE9NUFAtYFRts1qMeVWPYMrkjJWNLVNS4RAmuw7BlccZyfpKvLXgLWXovKwOp269GWPYPvUtvw/fb4HaTe/pB0v0u/W2kavKCDDXu0Zc/gy9wJKeu+9Whx0CVCYDGWPQPG+x2wNl1UBha1Z4+27Bk8zd2e0yelok1f3cxgAYuy7BmsICU5BUxXt8+jiUVlYCV6tGXP2FnE/JHl4GasmOnqdjiyWFQGVqJHw87ajQD98s1LzAobrZ6OB8fqm24shoH0aNhZuxEgdNAZ5fgeLIvKwEr0aNiZEaCr/wM+azyy6f6ocwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=800x800>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
